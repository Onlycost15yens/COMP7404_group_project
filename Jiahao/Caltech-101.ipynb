{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Caltect-101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "\n",
    "# # Download latest version\n",
    "# path = kagglehub.dataset_download(\"imbikramsaha/caltech-101\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [00:01<00:00, 28.71it/s]\n",
      "100%|██████████| 64/64 [00:02<00:00, 27.04it/s]\n",
      "100%|██████████| 798/798 [00:27<00:00, 28.79it/s]\n",
      "100%|██████████| 34/34 [00:01<00:00, 28.87it/s]\n",
      "100%|██████████| 435/435 [00:15<00:00, 27.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature set 1: X shape (1439, 48)\n",
      "Feature set 2: X shape (1439, 40)\n",
      "Feature set 3: X shape (1439, 254)\n",
      "Feature set 4: X shape (1439, 1984)\n",
      "Feature set 5: X shape (1439, 512)\n",
      "Feature set 6: X shape (1439, 928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pywt\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from skimage import color, transform, filters\n",
    "import os\n",
    "from skimage.feature import hog, local_binary_pattern\n",
    "from scipy.ndimage import convolve\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Function to extract Gabor features\n",
    "def extract_gabor(image):\n",
    "    kernels = []\n",
    "    # 6 orientations x 4 wavelengths = 24 kernels\n",
    "    for theta in np.linspace(0, np.pi, 6):  # 6 orientations\n",
    "        for lambd in [4, 8, 12, 16]:        # 4 wavelengths\n",
    "            kernel = cv2.getGaborKernel((31, 31), 4.0, theta, lambd, 0.5, 0, ktype=cv2.CV_32F)\n",
    "            kernels.append(kernel)\n",
    "    \n",
    "    features = []\n",
    "    for kernel in kernels:\n",
    "        filtered = cv2.filter2D(image, cv2.CV_32F, kernel)\n",
    "        features.extend([np.mean(filtered), np.std(filtered)])\n",
    "    \n",
    "    return np.array(features)[:48]\n",
    "\n",
    "# Function to extract Wavelet-Moment features (40-D)\n",
    "def extract_wavelet_moment(image):\n",
    "    # 3-level wavelet decomposition\n",
    "    coeffs1 = pywt.dwt2(image, 'haar')\n",
    "    cA1, (cH1, cV1, cD1) = coeffs1\n",
    "    coeffs2 = pywt.dwt2(cA1, 'haar')\n",
    "    cA2, (cH2, cV2, cD2) = coeffs2\n",
    "    coeffs3 = pywt.dwt2(cA2, 'haar')\n",
    "    cA3, (cH3, cV3, cD3) = coeffs3\n",
    "\n",
    "    # Collect all 10 subbands (9 details + 1 approx)\n",
    "    subbands = [cH1, cV1, cD1, cH2, cV2, cD2, cH3, cV3, cD3, cA3]\n",
    "    \n",
    "    features = []\n",
    "    for sub in subbands:\n",
    "        features.extend([np.mean(sub), np.var(sub), \n",
    "                        stats.skew(sub.ravel()), stats.kurtosis(sub.ravel())])\n",
    "    \n",
    "    return np.array(features)[:40]  # Ensure 40-D\n",
    "\n",
    "# Function to extract CENTRIST features (254-D) - Placeholder\n",
    "def extract_centrist(image):\n",
    "    # Census Transform with 3x3 neighborhood\n",
    "    centrist = np.zeros_like(image, dtype=np.uint8)\n",
    "    for y in range(1, image.shape[0]-1):\n",
    "        for x in range(1, image.shape[1]-1):\n",
    "            bits = 0\n",
    "            center = image[y,x]\n",
    "            for i, (dy, dx) in enumerate([(-1,-1), (-1,0), (-1,1),\n",
    "                                         (0,1), (1,1), (1,0), \n",
    "                                         (1,-1), (0,-1)]):\n",
    "                bits |= (1 << i) if image[y+dy, x+dx] > center else 0\n",
    "            centrist[y,x] = bits\n",
    "    \n",
    "    # Exclude first and last bins to get 254-D\n",
    "    hist = np.histogram(centrist, bins=256, range=(0,256))[0]\n",
    "    return hist[1:-1]  # 254 elements\n",
    "\n",
    "def extract_hog(image):\n",
    "    # Special parameters to achieve 1984-D\n",
    "    fd = hog(image, orientations=8, pixels_per_cell=(16,16),\n",
    "             cells_per_block=(3,3), visualize=False, \n",
    "             feature_vector=True, channel_axis=None)\n",
    "    \n",
    "    # Zero-pad or truncate to exact 1984-D\n",
    "    return np.resize(fd, 1984)\n",
    "\n",
    "# Function to extract GIST features (512-D)\n",
    "def extract_gist(image):\n",
    "    # Simplified spatial envelope approach\n",
    "    fft = np.fft.fft2(image)\n",
    "    magnitude = np.log1p(np.abs(fft))\n",
    "    \n",
    "    # Downsample and select components\n",
    "    gist = cv2.resize(magnitude, (32, 32)).flatten()\n",
    "    return gist[:512]\n",
    "\n",
    "def extract_lbp(image):\n",
    "    # Multi-region uniform LBP\n",
    "    radius = 3\n",
    "    n_points = 24\n",
    "    grid = (4, 4)  # 16 regions\n",
    "    \n",
    "    height, width = image.shape\n",
    "    features = []\n",
    "    \n",
    "    # Divide image into 4x4 grid\n",
    "    for i in range(grid[0]):\n",
    "        for j in range(grid[1]):\n",
    "            y_start = i * height//grid[0]\n",
    "            y_end = (i+1) * height//grid[0]\n",
    "            x_start = j * width//grid[1]\n",
    "            x_end = (j+1) * width//grid[1]\n",
    "            \n",
    "            patch = image[y_start:y_end, x_start:x_end]\n",
    "            lbp = local_binary_pattern(patch, n_points, radius, 'uniform')\n",
    "            hist = np.histogram(lbp, bins=59, range=(0, 59))[0]\n",
    "            features.extend(hist)\n",
    "    \n",
    "    return np.array(features)[:928]  # Ensure 928-D\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_and_extract_features(dataset_path):\n",
    "    # List to store features and labels\n",
    "    features_list = [[] for _ in range(6)]\n",
    "    labels = []\n",
    "\n",
    "    # Read dataset\n",
    "    print(\"reading dataset...\")\n",
    "    for label in os.listdir(dataset_path):\n",
    "        label_path = os.path.join(dataset_path, label)\n",
    "        if os.path.isdir(label_path) and label in [\"Faces\", \"Motorbikes\", \"dollar_bill\", \"garfield\", \"stop_sign\", \"windsor_chair\"]:\n",
    "            for image_name in tqdm(os.listdir(label_path)):\n",
    "                image_path = os.path.join(label_path, image_name)\n",
    "                if image_path.endswith('.jpg') or image_path.endswith('.png'):\n",
    "                    # Load the image and preprocess\n",
    "                    image = cv2.imread(image_path)\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "                    image = cv2.resize(image, (128, 128))  # Resize image\n",
    "                    \n",
    "                    # Extract features\n",
    "                    gabor_features = extract_gabor(image)\n",
    "                    wavelet_features = extract_wavelet_moment(image)\n",
    "                    centrist_features = extract_centrist(image)\n",
    "                    hog_features = extract_hog(image)\n",
    "                    gist_features = extract_gist(image)\n",
    "                    lbp_features = extract_lbp(image).flatten()\n",
    "\n",
    "                    # Store the features in respective arrays\n",
    "                    features_list[0].append(gabor_features)\n",
    "                    features_list[1].append(wavelet_features)\n",
    "                    features_list[2].append(centrist_features)\n",
    "                    features_list[3].append(hog_features)\n",
    "                    features_list[4].append(gist_features)\n",
    "                    features_list[5].append(lbp_features)\n",
    "\n",
    "                    # Store the label\n",
    "                    labels.append(label)\n",
    "    \n",
    "    # Convert feature list to numpy arrays\n",
    "    features_list = [np.array(feature) for feature in features_list]\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return features_list, labels\n",
    "\n",
    "# Example usage\n",
    "dataset_path = 'caltech-101'\n",
    "feature_list, labels = load_and_extract_features(dataset_path)\n",
    "\n",
    "for i, X in enumerate(feature_list):\n",
    "    print(f\"Feature set {i+1}: X shape {X.shape}\")\n",
    "    \n",
    "class_number = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Single view graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from tqdm import tqdm \n",
    "import cvxpy as cp\n",
    "\n",
    "def compute_laplacian(S):\n",
    "    S_sym = (S.T + S) / 2  # Compute symmetric part\n",
    "    D = np.diag(S_sym.sum(axis=0))  # Compute diagonal matrix D\n",
    "    L = D - S_sym  # Compute Laplacian matrix\n",
    "    return L\n",
    "\n",
    "def update_Q(L, c):\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(L)\n",
    "    Q = eigenvectors[:, :c]\n",
    "    return Q\n",
    "\n",
    "def update_S(Q, beta): # equation (9)\n",
    "    n, c = Q.shape\n",
    "    S = np.zeros((n, n))\n",
    "    \n",
    "    for j in range(n):\n",
    "        g_j = np.array([np.linalg.norm(Q[j] - Q[i])**2 for i in range(n)])\n",
    "        intermediate = g_j / (2 * beta)\n",
    "        eta = (1 + np.sum(intermediate)) / n\n",
    "        s_j = np.maximum(0, -intermediate + eta)\n",
    "        \n",
    "        S[j] = s_j\n",
    "    \n",
    "    return S\n",
    "\n",
    "def make_single_view_graph(single_view_graph_X, class_number, default_beta=1.0):\n",
    "    \n",
    "    single_view_graph = []\n",
    "    \n",
    "    for i in tqdm(range(len(single_view_graph_X))):\n",
    "        \n",
    "        # init\n",
    "        beta = default_beta\n",
    "        S = update_S(single_view_graph_X[i], beta)\n",
    "        L = compute_laplacian(S)\n",
    "        Q = update_Q(L, class_number)\n",
    "\n",
    "        for j in range(100):\n",
    "            S = update_S(Q, beta)\n",
    "            L = compute_laplacian(S)\n",
    "            Q = update_Q(L, class_number)\n",
    "\n",
    "            L_rank = np.linalg.matrix_rank(L)\n",
    "            # print(\"L_rank\",L_rank, \"beta\", beta)\n",
    "            if L_rank == X.shape[0] - class_number:\n",
    "                tqdm.write(f\"{i+1}th graph end at {j}th iteration, L's rank is {L_rank}\")\n",
    "                break\n",
    "            elif L_rank > X.shape[0] - class_number:\n",
    "                beta *= 0.9 \n",
    "            else:\n",
    "                beta *= 1.1\n",
    "                \n",
    "        single_view_graph.append(L)\n",
    "        \n",
    "    return single_view_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [54:45<00:00, 547.53s/it]\n"
     ]
    }
   ],
   "source": [
    "single_view_graph = make_single_view_graph(feature_list, class_number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Global view graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_W(single_view_graph):\n",
    "    W = [np.full(single_view_graph[0].shape, 1/len(single_view_graph))] * len(single_view_graph)\n",
    "    return W\n",
    "\n",
    "def init_A(single_view_graph, W):\n",
    "    A = np.sum(single_view_graph, axis=0) * W[0]\n",
    "    return A\n",
    "\n",
    "def init_P(A,c):\n",
    "    L = compute_laplacian(A)\n",
    "    P = update_Q(L, c)\n",
    "    return P\n",
    "\n",
    "def update_A(P, w_list, s_list, gamma=1.0):\n",
    "    n = P.shape[0]\n",
    "    c = P.shape[1]\n",
    "    m = len(w_list)\n",
    "\n",
    "    H = np.sum((P[:, np.newaxis, :] - P)**2, axis=2)\n",
    "    \n",
    "    A = np.zeros((n, n))\n",
    "    \n",
    "    for j in range(c):\n",
    "        h_j = H[:, j]\n",
    "    \n",
    "        sum_term = np.zeros(n)\n",
    "        for v in range(m):\n",
    "            w_jv = w_list[v][:, j]\n",
    "            s_jv = s_list[v][:, j] \n",
    "            sum_term += w_jv * s_jv  \n",
    "        intermediate = (((gamma / 2.0) * (h_j)) - sum_term)\n",
    "        \n",
    "    eta = (1 + np.sum(intermediate)) / n\n",
    "    a_j = np.maximum(0, -intermediate + eta)\n",
    "    \n",
    "    A[j] = a_j\n",
    "\n",
    "    return A\n",
    "\n",
    "\n",
    "def update_P(L, c):\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(L)\n",
    "    Q = eigenvectors[:, :c]\n",
    "    return Q\n",
    "\n",
    "\n",
    "def compute_W(a, s_list):\n",
    "    v, n, _ = np.shape(s_list) \n",
    "    w_list = []\n",
    "\n",
    "    for i in range(v):\n",
    "        wv = np.zeros((n, n)) \n",
    "        for j in range(n):\n",
    "            Z_j = a[:,j] - s_list[i][:,j]\n",
    "            Z_j = Z_j.reshape(1, -1) \n",
    "            one_vector = np.ones((n, 1)) \n",
    "            \n",
    "            # try: # takes forever\n",
    "            #     print(\"not triggered\", j)\n",
    "            #     ZTZ_inv = np.linalg.pinv(Z_j.T @ Z_j)  # (Z_j^T Z_j)^{-1}\n",
    "            #     w_jv = (ZTZ_inv @ one_vector) * (1 / (one_vector.T @ ZTZ_inv @ one_vector))\n",
    "            # except:\n",
    "            #     print(\"triggered\", j)\n",
    "            #     w_jv = np.zeros((1, n))\n",
    "                \n",
    "            sum_Z = np.sum(Z_j)\n",
    "            if np.isclose(sum_Z, 0.0):\n",
    "                w_jv = np.zeros((1, n))\n",
    "            else:\n",
    "                w_jv = Z_j.T / sum_Z\n",
    "\n",
    "            wv[:,j] = w_jv.reshape(-1) / np.sum(w_jv)\n",
    "        w_list.append(wv)\n",
    "\n",
    "    return w_list\n",
    "\n",
    "def make_global_graph(single_view_graph, class_number, default_gamma=1.0):\n",
    "    \n",
    "    # init\n",
    "    W = init_W(single_view_graph)\n",
    "    A = init_A(single_view_graph, W)\n",
    "    P = init_P(A, class_number)\n",
    "    gamma = default_gamma\n",
    "    \n",
    "    for j in tqdm(range(100)):\n",
    "        A = update_A(P, W, single_view_graph)\n",
    "        L = compute_laplacian(A)\n",
    "        P = update_P(L, class_number)\n",
    "        W = compute_W(A, single_view_graph)\n",
    "  \n",
    "        L_rank = np.linalg.matrix_rank(L)\n",
    "        tqdm.write(f\"iteration: {j}, L_rank: {L_rank}, gamma: {gamma}\")\n",
    "        if L_rank == X.shape[0] - class_number:\n",
    "            tqdm.write(f\"end at {j}th iteration, L's rank is {L_rank}\")\n",
    "            break\n",
    "        elif L_rank < X.shape[0] - class_number:\n",
    "            gamma *= 0.9 \n",
    "        else:\n",
    "            gamma *= 1.1\n",
    "        \n",
    "    return L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/var/folders/r8/ppzf44gs3hx3xh8q6lfht60r0000gn/T/ipykernel_88699/3323898247.py:72: RuntimeWarning: invalid value encountered in divide\n",
      "  wv[:,j] = w_jv.reshape(-1) / np.sum(w_jv)\n",
      "  1%|          | 1/100 [00:02<04:01,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0, L_rank: 0, gamma: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:04<08:09,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1, L_rank: 1433, gamma: 0.9\n",
      "end at 1th iteration, L's rank is 1433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "global_graph = make_global_graph(single_view_graph, class_number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def cluster(laplacian, n_clusters):\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(laplacian)\n",
    "    X = eigenvectors[:, :n_clusters]\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(X)\n",
    "    return kmeans.labels_\n",
    "\n",
    "# get clustering results\n",
    "single_view_graph_labels = []\n",
    "for i in range(len(single_view_graph)):\n",
    "    single_view_graph_labels.append(cluster(single_view_graph[i], class_number))\n",
    "\n",
    "global_graph_labels = cluster(global_graph, class_number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "\n",
    "def cluster_accuracy(y_true, y_pred):\n",
    "    acc = np.mean(y_pred == y_true)\n",
    "    return acc\n",
    "\n",
    "def purity_score(y_true, y_pred):\n",
    "    contingency = contingency_matrix(y_true, y_pred)\n",
    "    return np.sum(np.amax(contingency, axis=0)) / np.sum(contingency)\n",
    "\n",
    "def pairwise_precision_recall_fscore(y_true, y_pred):\n",
    "\n",
    "    def get_pairs(labels):\n",
    "        pairs = set()\n",
    "        for label in np.unique(labels):\n",
    "            indices = np.where(labels == label)[0]\n",
    "            for i in range(len(indices)):\n",
    "                for j in range(i + 1, len(indices)):\n",
    "                    pairs.add((indices[i], indices[j]))\n",
    "        return pairs\n",
    "\n",
    "    true_pairs = get_pairs(y_true)\n",
    "    pred_pairs = get_pairs(y_pred)\n",
    "    \n",
    "    tp = len(true_pairs & pred_pairs)\n",
    "    fp = len(pred_pairs - true_pairs)\n",
    "    fn = len(true_pairs - pred_pairs)\n",
    "\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f_score = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    \n",
    "    return precision, recall, f_score\n",
    "\n",
    "def evaluate_clustering(y_true, y_pred):\n",
    "    \n",
    "    # remapping \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    assert y_true.shape == y_pred.shape\n",
    "\n",
    "    labels = np.unique(y_true)\n",
    "    pred_labels = np.unique(y_pred)\n",
    "    cost_matrix = -contingency_matrix(y_true, y_pred)\n",
    "\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    best_mapping = {pred_labels[col]: labels[row] for row, col in zip(row_ind, col_ind)}\n",
    "\n",
    "    y_pred_mapped = np.array([best_mapping[label] for label in y_pred])\n",
    "\n",
    "    # evaluate\n",
    "    acc = cluster_accuracy(y_true, y_pred_mapped)\n",
    "    nmi = normalized_mutual_info_score(y_true, y_pred)\n",
    "    purity = purity_score(y_true, y_pred_mapped)\n",
    "    precision, recall, f_score = pairwise_precision_recall_fscore(y_true, y_pred_mapped)\n",
    "    ari = adjusted_rand_score(y_true, y_pred_mapped)\n",
    "\n",
    "    return {\n",
    "        \"ACC\": acc,\n",
    "        \"NMI\": nmi,\n",
    "        \"Purity\": purity,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F-score\": f_score,\n",
    "        \"ARI\": ari\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ACC': 0.5552466990965949, 'NMI': 0.01966058694104668, 'Purity': 0.5580264072272412, 'Precision': 0.40643489144600137, 'Recall': 0.9994136525616203, 'F-score': 0.5778667710969749, 'ARI': 0.008627131249400025}\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate_clustering(labels, global_graph_labels)\n",
    "print(metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
