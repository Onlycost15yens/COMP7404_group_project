{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Caltect-101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfds\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskimage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hog, local_binary_pattern\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_datasets'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import cv2\n",
    "from skimage.feature import hog, local_binary_pattern\n",
    "from skimage import color\n",
    "import pywt\n",
    "from scipy import stats\n",
    "from gist import extract_gist_feature  # Requires installation: pip install git+https://github.com/tuttieee/lear-gist-python.git\n",
    "\n",
    "# Function to extract Gabor features (48-D)\n",
    "def extract_gabor(img_gray):\n",
    "    features = []\n",
    "    for theta in np.linspace(0, np.pi, 6):  # 6 orientations\n",
    "        for sigma in [1, 2, 3, 4]:         # 4 scales\n",
    "            filt_real, filt_imag = pywt.filters.gabor(img_gray, frequency=1.0/sigma, theta=theta)\n",
    "            magnitude = np.sqrt(filt_real**2 + filt_imag**2)\n",
    "            features.append(np.mean(magnitude))\n",
    "            features.append(np.std(magnitude))\n",
    "    return np.array(features)[:48]  # Ensure 48-D\n",
    "\n",
    "# Function to extract Wavelet-Moment features (40-D)\n",
    "def extract_wavelet_moment(img_gray):\n",
    "    coeffs = pywt.wavedec2(img_gray, 'db1', level=3)\n",
    "    features = []\n",
    "    # Process Approximation (Level 3)\n",
    "    cA = coeffs[0]\n",
    "    features.extend([np.mean(cA), np.var(cA), stats.skew(cA.ravel()), stats.kurtosis(cA.ravel())])\n",
    "    # Process Details (Levels 1-3)\n",
    "    for detail in coeffs[1:]:\n",
    "        for d in detail:\n",
    "            flattened = d.ravel()\n",
    "            features.extend([np.mean(flattened), np.var(flattened), stats.skew(flattened), stats.kurtosis(flattened)])\n",
    "    return np.array(features)[:40]  # Truncate to 40-D\n",
    "\n",
    "# Function to extract CENTRIST features (254-D) - Placeholder\n",
    "def extract_centrist(img_gray):\n",
    "    # Implement custom CENTRIST feature extraction here\n",
    "    return np.zeros(254)  # Replace with actual implementation\n",
    "\n",
    "# Function to extract HOG features (1984-D)\n",
    "def extract_hog(img_gray):\n",
    "    fd = hog(img_gray, orientations=9, pixels_per_cell=(16, 16),\n",
    "             cells_per_block=(3, 3), feature_vector=True)\n",
    "    # Adjust parameters to reach 1984-D if necessary\n",
    "    return fd[:1984] if len(fd) >= 1984 else np.pad(fd, (0, 1984 - len(fd)))\n",
    "\n",
    "# Function to extract GIST features (512-D)\n",
    "def extract_gist(img_rgb):\n",
    "    gist_feat = extract_gist_feature(img_rgb)\n",
    "    return gist_feat[:512]  # Ensure 512-D\n",
    "\n",
    "# Function to extract LBP features (928-D)\n",
    "def extract_lbp(img_gray):\n",
    "    lbp = local_binary_pattern(img_gray, P=24, R=3, method='uniform')\n",
    "    hist, _ = np.histogram(lbp, bins=59, range=(0, 58))\n",
    "    # Spatial pyramid (4x4 grid)\n",
    "    height, width = lbp.shape\n",
    "    hist_features = []\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            cell = lbp[i*height//4:(i+1)*height//4, j*width//4:(j+1)*width//4]\n",
    "            cell_hist, _ = np.histogram(cell, bins=59, range=(0, 58))\n",
    "            hist_features.extend(cell_hist)\n",
    "    return np.array(hist_features)[:928]  # Ensure 928-D\n",
    "\n",
    "# Main processing\n",
    "def process_caltech101():\n",
    "    # Load dataset\n",
    "    ds, info = tfds.load('caltech101', split='train', shuffle_files=True, with_info=True)\n",
    "    images, labels = [], []\n",
    "\n",
    "    # Collect data\n",
    "    for example in tfds.as_numpy(ds):\n",
    "        images.append(example['image'])\n",
    "        labels.append(example['label'])\n",
    "\n",
    "    # Preprocess and extract features\n",
    "    feature_list = [[] for _ in range(6)]\n",
    "    label_list = []\n",
    "\n",
    "    for img, label in zip(images, labels):\n",
    "        # Resize and convert to RGB & Grayscale\n",
    "        img_rgb = cv2.resize(img, (128, 128))  # Resize for consistency\n",
    "        img_gray = color.rgb2gray(img_rgb) * 255\n",
    "        img_gray = img_gray.astype(np.uint8)\n",
    "\n",
    "        # Extract features\n",
    "        feature_list[0].append(extract_gabor(img_gray))\n",
    "        feature_list[1].append(extract_wavelet_moment(img_gray))\n",
    "        feature_list[2].append(extract_centrist(img_gray))\n",
    "        feature_list[3].append(extract_hog(img_gray))\n",
    "        feature_list[4].append(extract_gist(img_rgb))\n",
    "        feature_list[5].append(extract_lbp(img_gray))\n",
    "        label_list.append(label)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    feature_list = [np.array(lst) for lst in feature_list]\n",
    "    label_list = np.array(label_list)\n",
    "\n",
    "    return feature_list, label_list\n",
    "\n",
    "# Execute\n",
    "feature_list, label_list = process_caltech101()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Single view graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from tqdm import tqdm \n",
    "\n",
    "def estimate_sigma(X):\n",
    "    pairwise_dists = np.linalg.norm(X[:, np.newaxis] - X, axis=2)  # Compute pairwise L2 distances\n",
    "    sigma = np.median(pairwise_dists)  # Use the median distance as sigma\n",
    "    return sigma\n",
    "\n",
    "def compute_laplacian(S):\n",
    "    S_sym = (S.T + S) / 2  # Compute symmetric part\n",
    "    D = np.diag(S_sym.sum(axis=0))  # Compute diagonal matrix D\n",
    "    L = D - S_sym  # Compute Laplacian matrix\n",
    "    return L\n",
    "\n",
    "def update_Q(L, c):\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(L)\n",
    "    Q = eigenvectors[:, :c]\n",
    "    return Q\n",
    "\n",
    "def project_to_simplex(v):# equation (9)\n",
    "    n = len(v)\n",
    "    u = np.sort(v)[::-1] \n",
    "    cumsum_u = np.cumsum(u)\n",
    "    rho = np.where(u > (cumsum_u - 1) / (np.arange(n) + 1))[0][-1]\n",
    "    theta = (cumsum_u[rho] - 1) / (rho + 1)\n",
    "    return np.maximum(v - theta, 0)\n",
    "\n",
    "def update_S(Q, beta): # equation (9)\n",
    "    n, c = Q.shape\n",
    "    S = np.zeros((n, n))\n",
    "    \n",
    "    for j in range(n):\n",
    "        g_j = np.array([np.linalg.norm(Q[j] - Q[i])**2 for i in range(n)])\n",
    "        raw_sj = -g_j / (2 * beta)\n",
    "        S[j] = project_to_simplex(raw_sj)\n",
    "    \n",
    "    return S\n",
    "\n",
    "def make_single_view_graph(single_view_graph_X, class_number, default_beta=1.0):\n",
    "    \n",
    "    single_view_graph = []\n",
    "    \n",
    "    for i in tqdm(range(len(single_view_graph_X))):\n",
    "        \n",
    "        # init\n",
    "        beta = default_beta\n",
    "        S = update_S(single_view_graph_X[i], beta)\n",
    "        L = compute_laplacian(S)\n",
    "        Q = update_Q(L, class_number)\n",
    "\n",
    "        for j in tqdm(range(100)):\n",
    "            S = update_S(Q, beta)\n",
    "            L = compute_laplacian(S)\n",
    "            Q = update_Q(L, class_number)\n",
    "\n",
    "            L_rank = np.linalg.matrix_rank(L)\n",
    "            if L_rank == X.shape[0] - class_number:\n",
    "                tqdm.write(f\"{i}th graph end at {j}th iteration, L's rank is {L_rank}\")\n",
    "                break\n",
    "            elif L_rank > X.shape[0] - class_number:\n",
    "                beta *= 0.9 \n",
    "            else:\n",
    "                beta *= 1.1\n",
    "        single_view_graph.append(L)\n",
    "        \n",
    "    return single_view_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     \n",
      " 12%|█▏        | 12/100 [02:53<21:13, 14.47s/it]\n",
      " 17%|█▋        | 1/6 [03:01<15:08, 181.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th graph end at 12th iteration, L's rank is 1990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      "  7%|▋         | 7/100 [01:40<22:14, 14.35s/it]\n",
      " 33%|███▎      | 2/6 [04:50<09:15, 138.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th graph end at 7th iteration, L's rank is 1990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "single_view_graph = make_single_view_graph(feature_list, class_number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Global view graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_W(single_view_graph):\n",
    "    W = [np.full(single_view_graph[0].shape, 1/len(single_view_graph))] * len(single_view_graph)\n",
    "    return W\n",
    "\n",
    "def init_A(single_view_graph, W):\n",
    "    A = np.sum(single_view_graph, axis=0) * W[0]\n",
    "    return A\n",
    "\n",
    "def init_P(A,c):\n",
    "    L = compute_laplacian(A)\n",
    "    P = update_Q(L, c)\n",
    "    return P\n",
    "\n",
    "def update_A(P, w_list, s_list, gamma=1.0):\n",
    "    n = P.shape[0]\n",
    "    c = P.shape[1]\n",
    "    m = len(w_list)\n",
    "\n",
    "    H = np.sum((P[:, np.newaxis, :] - P)**2, axis=2)\n",
    "    \n",
    "    A = np.zeros((n, n))\n",
    "    \n",
    "    for j in range(c):\n",
    "        h_j = H[:, j]\n",
    "    \n",
    "        sum_term = np.zeros(n)\n",
    "        for v in range(m):\n",
    "            w_jv = w_list[v][:, j]\n",
    "            s_jv = s_list[v][:, j] \n",
    "            sum_term += w_jv * s_jv  \n",
    "        intermediate = -(((gamma / 2.0) * (h_j)) - sum_term)\n",
    "        \n",
    "    A[j] = project_to_simplex(intermediate)\n",
    "\n",
    "    return A\n",
    "\n",
    "\n",
    "def update_P(L, c):\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(L)\n",
    "    Q = eigenvectors[:, :c]\n",
    "    return Q\n",
    "\n",
    "\n",
    "def compute_W(a, s_list):\n",
    "    v, n, _ = np.shape(s_list) \n",
    "    w_list = []\n",
    "\n",
    "    for i in range(v):\n",
    "        wv = np.zeros((n, n)) \n",
    "        for j in range(n):\n",
    "            Z_j = a[:,j] - s_list[i][:,j]\n",
    "            Z_j = Z_j.reshape(1, -1) \n",
    "            ZTZ_inv = np.linalg.pinv(Z_j.T @ Z_j)  # (Z_j^T Z_j)^{-1}\n",
    "            one_vector = np.ones((n, 1)) \n",
    "            w_jv = (ZTZ_inv @ one_vector) * (1 / (one_vector.T @ ZTZ_inv @ one_vector))\n",
    "            wv[:,j] = w_jv.reshape(-1) / np.sum(w_jv)\n",
    "        w_list.append(wv)\n",
    "\n",
    "    return w_list\n",
    "\n",
    "def make_global_graph(single_view_graph, class_number, default_gamma=1.0):\n",
    "    \n",
    "    # init\n",
    "    W = init_W(single_view_graph)\n",
    "    A = init_A(single_view_graph, W)\n",
    "    P = init_P(A, class_number)\n",
    "    gamma = default_gamma\n",
    "    \n",
    "    for j in tqdm(range(100)):\n",
    "        A = update_A(P, W, single_view_graph)\n",
    "        L = compute_laplacian(A)\n",
    "        P = update_P(L, class_number)\n",
    "        W = compute_W(A, single_view_graph)\n",
    "\n",
    "        tqdm.write(f\"iteration: {j}, L_rank: {np.linalg.matrix_rank(L)}, gamma: {gamma}\")\n",
    "        print(sum(W[0][:,0]))\n",
    "        L_rank = np.linalg.matrix_rank(L)\n",
    "        if L_rank == X.shape[0] - class_number:\n",
    "            tqdm.write(f\"end at {j}th iteration, L's rank is {L_rank}\")\n",
    "            break\n",
    "        elif L_rank < X.shape[0] - class_number:\n",
    "            gamma *= 0.9 \n",
    "        else:\n",
    "            gamma *= 1.1\n",
    "        \n",
    "    return L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_graph = make_global_graph(single_view_graph, class_number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def cluster(laplacian, n_clusters):\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(laplacian)\n",
    "    X = eigenvectors[:, :n_clusters]\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(X)\n",
    "    return kmeans.labels_\n",
    "\n",
    "# get clustering results\n",
    "single_view_graph_labels = []\n",
    "for i in range(len(single_view_graph)):\n",
    "    single_view_graph_labels.append(cluster(single_view_graph[i], class_number))\n",
    "\n",
    "global_graph_labels = cluster(global_graph, class_number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "\n",
    "def cluster_accuracy(y_true, y_pred):\n",
    "    acc = np.mean(y_pred == y_true)\n",
    "    return acc\n",
    "\n",
    "def purity_score(y_true, y_pred):\n",
    "    contingency = contingency_matrix(y_true, y_pred)\n",
    "    return np.sum(np.amax(contingency, axis=0)) / np.sum(contingency)\n",
    "\n",
    "def pairwise_precision_recall_fscore(y_true, y_pred):\n",
    "\n",
    "    def get_pairs(labels):\n",
    "        pairs = set()\n",
    "        for label in np.unique(labels):\n",
    "            indices = np.where(labels == label)[0]\n",
    "            for i in range(len(indices)):\n",
    "                for j in range(i + 1, len(indices)):\n",
    "                    pairs.add((indices[i], indices[j]))\n",
    "        return pairs\n",
    "\n",
    "    true_pairs = get_pairs(y_true)\n",
    "    pred_pairs = get_pairs(y_pred)\n",
    "    \n",
    "    tp = len(true_pairs & pred_pairs)\n",
    "    fp = len(pred_pairs - true_pairs)\n",
    "    fn = len(true_pairs - pred_pairs)\n",
    "\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f_score = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    \n",
    "    return precision, recall, f_score\n",
    "\n",
    "def evaluate_clustering(y_true, y_pred):\n",
    "    \n",
    "    # remapping \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    assert y_true.shape == y_pred.shape\n",
    "\n",
    "    labels = np.unique(y_true)\n",
    "    pred_labels = np.unique(y_pred)\n",
    "    cost_matrix = -contingency_matrix(y_true, y_pred)\n",
    "\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    best_mapping = {pred_labels[col]: labels[row] for row, col in zip(row_ind, col_ind)}\n",
    "\n",
    "    y_pred_mapped = np.array([best_mapping[label] for label in y_pred])\n",
    "\n",
    "    # evaluate\n",
    "    acc = cluster_accuracy(y_true, y_pred_mapped)\n",
    "    nmi = normalized_mutual_info_score(y_true, y_pred)\n",
    "    purity = purity_score(y_true, y_pred_mapped)\n",
    "    precision, recall, f_score = pairwise_precision_recall_fscore(y_true, y_pred_mapped)\n",
    "    ari = adjusted_rand_score(y_true, y_pred_mapped)\n",
    "\n",
    "    return {\n",
    "        \"ACC\": acc,\n",
    "        \"NMI\": nmi,\n",
    "        \"Purity\": purity,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F-score\": f_score,\n",
    "        \"ARI\": ari\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = evaluate_clustering(y, global_graph_labels)\n",
    "print(metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
