{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 UCI Digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mfeat-fou already downloaded.\n",
      "mfeat-fac already downloaded.\n",
      "mfeat-kar already downloaded.\n",
      "mfeat-pix already downloaded.\n",
      "mfeat-zer already downloaded.\n",
      "mfeat-mor already downloaded.\n",
      "Feature set 1: X shape (2000, 76), y shape (2000,)\n",
      "Feature set 2: X shape (2000, 216), y shape (2000,)\n",
      "Feature set 3: X shape (2000, 64), y shape (2000,)\n",
      "Feature set 4: X shape (2000, 240), y shape (2000,)\n",
      "Feature set 5: X shape (2000, 47), y shape (2000,)\n",
      "Feature set 6: X shape (2000, 6), y shape (2000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# Define dataset URL and filenames\n",
    "BASE_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/mfeat/\"\n",
    "FILENAMES = {\n",
    "    \"profile\": \"mfeat-fou\",\n",
    "    \"fourier\": \"mfeat-fac\",\n",
    "    \"karhunen\": \"mfeat-kar\",\n",
    "    \"intensity\": \"mfeat-pix\",\n",
    "    \"zernike\": \"mfeat-zer\",\n",
    "    \"morphological\": \"mfeat-mor\"\n",
    "}\n",
    "\n",
    "# Define feature dimensions\n",
    "FEATURE_DIMS = {\n",
    "    \"profile\": 216,\n",
    "    \"fourier\": 76,\n",
    "    \"karhunen\": 64,\n",
    "    \"intensity\": 240,\n",
    "    \"zernike\": 47,\n",
    "    \"morphological\": 6\n",
    "}\n",
    "\n",
    "# Create a directory for the dataset\n",
    "DATA_DIR = \"uci_digits\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Function to download the dataset\n",
    "def download_data():\n",
    "    for key, filename in FILENAMES.items():\n",
    "        file_path = os.path.join(DATA_DIR, filename)\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Downloading {filename}...\")\n",
    "            urllib.request.urlretrieve(BASE_URL + filename, file_path)\n",
    "        else:\n",
    "            print(f\"{filename} already downloaded.\")\n",
    "download_data()\n",
    "\n",
    "# Load all six feature sets into a dictionary\n",
    "features = {key: np.loadtxt(os.path.join(DATA_DIR, filename)) for key, filename in FILENAMES.items()}\n",
    "\n",
    "# Create labels (0-9, 200 samples each)\n",
    "labels = np.repeat(np.arange(10), 200)\n",
    "\n",
    "# Convert dataset into a list of (X, y) pairs\n",
    "dataset_list = [(features[key], labels) for key in FILENAMES.keys()]\n",
    "\n",
    "# Verify dataset shapes\n",
    "feature_list = []\n",
    "for i, (X, y) in enumerate(dataset_list):\n",
    "    print(f\"Feature set {i+1}: X shape {X.shape}, y shape {y.shape}\")\n",
    "    feature_list.append(X)\n",
    "\n",
    "class_number = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Single view graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from tqdm import tqdm \n",
    "from scipy import linalg\n",
    "\n",
    "def compute_laplacian(S):\n",
    "    S_sym = (S.T + S) / 2  # Compute symmetric part\n",
    "    D = np.diag(S_sym.sum(axis=1))  # Compute diagonal matrix D\n",
    "    L = D - S_sym  # Compute Laplacian matrix\n",
    "    return L\n",
    "\n",
    "def update_Q(L, c):\n",
    "    L = (L + L.T) / 2\n",
    "    eigenvals, eigenvecs = linalg.eigh(L)\n",
    "    Q = eigenvecs[:, :c]\n",
    "    return Q\n",
    "\n",
    "\n",
    "def update_S(Q, beta):\n",
    "    n = Q.shape[0]\n",
    "    S = np.zeros((n, n))\n",
    "    \n",
    "    # Calculate pairwise distances (g_ij matrix)\n",
    "    G = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            G[i, j] = np.sum((Q[i] - Q[j]) ** 2)\n",
    "    \n",
    "    # Update each column of S independently\n",
    "    for j in range(n):\n",
    "        g_j = G[:, j]  # Get j-th column of G\n",
    "        \n",
    "        # Binary search for eta that satisfies sum(s_j) = 1\n",
    "        left, right = 0, np.max(g_j)/(2*beta)\n",
    "        \n",
    "        while right - left > 1e-10:\n",
    "            eta = (left + right) / 2\n",
    "            s_j = np.maximum(0, -g_j/(2*beta) + eta)\n",
    "            sum_s = np.sum(s_j)\n",
    "            \n",
    "            if sum_s < 1:\n",
    "                left = eta\n",
    "            else:\n",
    "                right = eta\n",
    "        \n",
    "        # Final update for j-th column using found eta\n",
    "        S[:, j] = np.maximum(0, -g_j/(2*beta) + eta)\n",
    "    \n",
    "    S = (S.T + S) / 2\n",
    "    return S\n",
    "\n",
    "\n",
    "def make_single_view_graph(single_view_graph_X, class_number, default_beta=1.0):\n",
    "    \n",
    "    single_view_graph = []\n",
    "    \n",
    "    for i in tqdm(range(len(single_view_graph_X))):\n",
    "        \n",
    "        # init\n",
    "        beta = default_beta\n",
    "        S = update_S(single_view_graph_X[i], beta)\n",
    "        L = compute_laplacian(S)\n",
    "        Q = update_Q(L, class_number)\n",
    "\n",
    "        for j in range(100):\n",
    "            S = update_S(Q, beta)\n",
    "            L = compute_laplacian(S)\n",
    "            Q = update_Q(L, class_number)\n",
    "\n",
    "            L_rank = np.linalg.matrix_rank(L)\n",
    "            # print(\"L_rank\",L_rank, \"beta\", beta)\n",
    "            if L_rank == X.shape[0] - class_number:\n",
    "                tqdm.write(f\"{i+1}th graph end at {j}th iteration, L's rank is {L_rank}\")\n",
    "                break\n",
    "            elif L_rank > X.shape[0] - class_number:\n",
    "                beta *= 0.9 \n",
    "            else:\n",
    "                beta *= 1.1\n",
    "                \n",
    "        single_view_graph.append(S)\n",
    "        \n",
    "    return single_view_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [04:00<20:04, 240.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th graph end at 12th iteration, L's rank is 1990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [06:25<12:17, 184.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2th graph end at 7th iteration, L's rank is 1990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [09:13<08:50, 176.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3th graph end at 7th iteration, L's rank is 1990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [11:49<05:36, 168.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4th graph end at 7th iteration, L's rank is 1990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [15:45<03:13, 193.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5th graph end at 11th iteration, L's rank is 1990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [19:30<00:00, 195.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6th graph end at 11th iteration, L's rank is 1990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "single_view_graph = make_single_view_graph(feature_list, class_number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Global view graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_W(single_view_graph):\n",
    "    W = [np.full(single_view_graph[0].shape, 1/len(single_view_graph))] * len(single_view_graph)\n",
    "    return W\n",
    "\n",
    "def init_A(single_view_graph, W):\n",
    "    A = np.sum(single_view_graph, axis=0) * W[0]\n",
    "    return A\n",
    "\n",
    "def init_P(A,c):\n",
    "    L = compute_laplacian(A)\n",
    "    P = update_Q(L, c)\n",
    "    return P\n",
    "\n",
    "\n",
    "def update_A(P, w_list, s_list, gamma=1.0):\n",
    "\n",
    "    n = P.shape[0]\n",
    "    A = np.zeros((n, n))\n",
    "    \n",
    "    # Compute hj for each column\n",
    "    H = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            H[i, j] = np.sum((P[i] - P[j])**2)  # ||p_i - p_j||^2\n",
    "    \n",
    "    # For each column j\n",
    "    for j in range(n):\n",
    "        # Compute the weighted sum of similarity matrices for column j\n",
    "        weighted_sum = np.zeros(n)\n",
    "        for v in range(len(w_list)):\n",
    "            weighted_sum += w_list[v][:, j] * s_list[v][:, j]\n",
    "        \n",
    "        # Compute b_j = hj - sum_v w_j^(v) * s_j^(v)\n",
    "        b_j = (gamma/2) *H[:, j] - weighted_sum\n",
    "        \n",
    "        # Update each column of S independently\n",
    "        for j in range(n):\n",
    "            \n",
    "            # Binary search for eta that satisfies sum(s_j) = 1\n",
    "            left, right = 0, np.max(b_j)\n",
    "            \n",
    "            while right - left > 1e-10:\n",
    "                eta = (left + right) / 2\n",
    "                s_j = np.maximum(0, -b_j + eta)\n",
    "                sum_s = np.sum(s_j)\n",
    "                \n",
    "                if sum_s < 1:\n",
    "                    left = eta\n",
    "                else:\n",
    "                    right = eta\n",
    "            \n",
    "            # Final update for j-th column using found eta\n",
    "            A[:, j] = np.maximum(0, -b_j + eta)\n",
    "    \n",
    "    A = (A.T + A) / 2\n",
    "    return A\n",
    "\n",
    "\n",
    "def update_P(L, c):\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(L)\n",
    "    Q = eigenvectors[:, :c]\n",
    "    return Q\n",
    "\n",
    "\n",
    "def project_simplex(v):\n",
    "    n = len(v)\n",
    "    \n",
    "    # Sort v in descending order\n",
    "    u = np.sort(v)[::-1]\n",
    "    \n",
    "    # Compute the threshold\n",
    "    cssv = np.cumsum(u) - 1\n",
    "    ind = np.arange(n) + 1\n",
    "    cond = u - cssv / ind > 0\n",
    "    \n",
    "    if not np.any(cond):  # Handle the case where all elements are negative\n",
    "        return np.ones(n) / n\n",
    "    \n",
    "    rho = ind[cond][-1]\n",
    "    \n",
    "    # Compute the Lagrange multiplier\n",
    "    theta = cssv[rho-1] / rho\n",
    "    \n",
    "    # Compute the projection\n",
    "    x = np.maximum(v - theta, 0)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def compute_W(a, s_list):\n",
    "    n = a.shape[0]\n",
    "    nv = len(s_list)  # Number of views\n",
    "    \n",
    "    # Initialize weight matrices\n",
    "    w_list = [np.zeros((n, n)) for _ in range(nv)]\n",
    "    \n",
    "    # For each column j\n",
    "    for j in range(n):\n",
    "        # Construct Z_j matrix where each column corresponds to a view\n",
    "        # and each row corresponds to a sample\n",
    "        Z_j = np.zeros((n, nv))\n",
    "        for v in range(nv):\n",
    "            Z_j[:, v] = a[:, j] - s_list[v][:, j]\n",
    "        \n",
    "        # Compute Z_j^T * Z_j\n",
    "        ZTZ = np.dot(Z_j.T, Z_j)\n",
    "        \n",
    "        # Handle potential numerical issues with matrix inversion\n",
    "        try:\n",
    "            # Add small regularization to ensure invertibility\n",
    "            epsilon = 1e-8\n",
    "            ZTZ_reg = ZTZ + epsilon * np.eye(nv)\n",
    "            \n",
    "            # Compute the inverse of Z_j^T * Z_j\n",
    "            ZTZ_inv = np.linalg.inv(ZTZ_reg)\n",
    "            \n",
    "            # Compute w_j according to the formula\n",
    "            ones = np.ones(nv)\n",
    "            denominator = np.dot(ones, np.dot(ZTZ_inv, ones))\n",
    "            \n",
    "            if np.abs(denominator) < 1e-10:  # Avoid division by zero\n",
    "                w_j = np.ones(nv) / nv  # Equal weights if denominator is close to zero\n",
    "            else:\n",
    "                w_j = np.dot(ZTZ_inv, ones) / denominator\n",
    "        except np.linalg.LinAlgError:\n",
    "            # If matrix is still singular, use equal weights\n",
    "            w_j = np.ones(nv) / nv\n",
    "        \n",
    "        # Handle numerical issues: ensure weights are non-negative\n",
    "        if np.any(w_j < 0):\n",
    "            # Project to the simplex if there are negative weights\n",
    "            w_j = project_simplex(w_j)\n",
    "        \n",
    "        # Ensure the constraint sum(w_j) = 1 is satisfied exactly\n",
    "        w_j = w_j / np.sum(w_j)\n",
    "        \n",
    "        # Store the weights in the weight matrices\n",
    "        for v in range(nv):\n",
    "            w_list[v][:, j] = w_j[v]\n",
    "            \n",
    "    return w_list\n",
    "\n",
    "def make_global_graph(single_view_graph, class_number, default_gamma=1.0):\n",
    "    \n",
    "    # init\n",
    "    W = init_W(single_view_graph)\n",
    "    A = init_A(single_view_graph, W)\n",
    "    P = init_P(A, class_number)\n",
    "    gamma = default_gamma\n",
    "    \n",
    "    for j in tqdm(range(5)):\n",
    "        A = update_A(P, W, single_view_graph)\n",
    "        L = compute_laplacian(A)\n",
    "        P = update_P(L, class_number)\n",
    "        W = compute_W(A, single_view_graph)\n",
    "\n",
    "        tqdm.write(f\"iteration: {j}, L_rank: {np.linalg.matrix_rank(L)}, gamma: {gamma}\")\n",
    "        # print(sum(W[0][:,0]))\n",
    "        L_rank = np.linalg.matrix_rank(L)\n",
    "        if L_rank == X.shape[0] - class_number:\n",
    "            tqdm.write(f\"end at {j}th iteration, L's rank is {L_rank}\")\n",
    "            break\n",
    "        elif L_rank < X.shape[0] - class_number:\n",
    "            gamma *= 0.9 \n",
    "        else:\n",
    "            gamma *= 1.1\n",
    "        \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [11:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0, L_rank: 1999, gamma: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [22:01<44:23, 665.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1, L_rank: 1999, gamma: 1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [32:57<33:06, 662.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 2, L_rank: 1999, gamma: 1.2100000000000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [44:04<21:59, 659.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 3, L_rank: 1999, gamma: 1.3310000000000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [55:18<11:02, 662.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 4, L_rank: 1999, gamma: 1.4641000000000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [55:23<00:00, 664.62s/it]\n"
     ]
    }
   ],
   "source": [
    "global_graph = make_global_graph(single_view_graph, class_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def cluster_from_similarity(similarity_matrix, method='spectral', n_clusters=None):\n",
    "    # 确保输入是方阵\n",
    "    assert similarity_matrix.shape[0] == similarity_matrix.shape[1], \"相似度矩阵必须是方阵\"\n",
    "    n = similarity_matrix.shape[0]\n",
    "    threshold = 1/n\n",
    "    \n",
    "    if method == 'connected_components':\n",
    "        # 基于阈值创建邻接矩阵\n",
    "        adjacency = (similarity_matrix >= threshold).astype(int)\n",
    "        # 使用连通分量算法进行聚类\n",
    "        n_components, labels = connected_components(adjacency, directed=False)\n",
    "        return labels\n",
    "    \n",
    "    elif method == 'spectral':\n",
    "        # 如果没有指定聚类数量，尝试自动估计\n",
    "        if n_clusters is None:\n",
    "            # 使用特征值差值估计聚类数量\n",
    "            laplacian = np.diag(np.sum(similarity_matrix, axis=1)) - similarity_matrix\n",
    "            eigvals = np.sort(np.linalg.eigvalsh(laplacian))[:10]  # 考虑前10个特征值\n",
    "            # 计算连续特征值的差值\n",
    "            gaps = np.diff(eigvals)\n",
    "            # 选择差值最大的位置作为聚类数量的估计\n",
    "            if len(gaps) > 0:\n",
    "                n_clusters = np.argmax(gaps) + 1\n",
    "                n_clusters = max(2, min(n_clusters, n // 2))  # 限制在合理范围内\n",
    "            else:\n",
    "                n_clusters = 2  # 默认为2个聚类\n",
    "        \n",
    "        # 构建归一化拉普拉斯矩阵\n",
    "        D = np.diag(np.sum(similarity_matrix, axis=1))\n",
    "        D_inv_sqrt = np.diag(1.0 / np.sqrt(np.maximum(np.diag(D), 1e-10)))\n",
    "        L = np.eye(n) - D_inv_sqrt @ similarity_matrix @ D_inv_sqrt\n",
    "        \n",
    "        # 计算特征值和特征向量\n",
    "        eigvals, eigvecs = np.linalg.eigh(L)\n",
    "        # 选择最小的n_clusters个特征值对应的特征向量\n",
    "        indices = np.argsort(eigvals)[:n_clusters]\n",
    "        features = eigvecs[:, indices]\n",
    "        \n",
    "        # 对特征向量进行行归一化\n",
    "        norm = np.sqrt(np.sum(features**2, axis=1)).reshape(-1, 1)\n",
    "        norm[norm == 0] = 1  # 避免除以零\n",
    "        features = features / norm\n",
    "        \n",
    "        # 使用K-means对特征向量进行聚类\n",
    "        kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "        labels = kmeans.fit_predict(features)\n",
    "        \n",
    "        return labels\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"不支持的聚类方法: {method}，请使用 'connected_components' 或 'spectral'\")\n",
    "\n",
    "\n",
    "# get clustering results\n",
    "single_view_graph_labels = []\n",
    "for i in range(len(single_view_graph)):\n",
    "    single_view_graph_labels.append(cluster_from_similarity(single_view_graph[i], n_clusters = class_number))\n",
    "\n",
    "global_graph_labels = cluster_from_similarity(global_graph, n_clusters = class_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "\n",
    "def cluster_accuracy(y_true, y_pred):\n",
    "    acc = np.mean(y_pred == y_true)\n",
    "    return acc\n",
    "\n",
    "def purity_score(y_true, y_pred):\n",
    "    contingency = contingency_matrix(y_true, y_pred)\n",
    "    return np.sum(np.amax(contingency, axis=0)) / np.sum(contingency)\n",
    "\n",
    "def pairwise_precision_recall_fscore(y_true, y_pred):\n",
    "\n",
    "    def get_pairs(labels):\n",
    "        pairs = set()\n",
    "        for label in np.unique(labels):\n",
    "            indices = np.where(labels == label)[0]\n",
    "            for i in range(len(indices)):\n",
    "                for j in range(i + 1, len(indices)):\n",
    "                    pairs.add((indices[i], indices[j]))\n",
    "        return pairs\n",
    "\n",
    "    true_pairs = get_pairs(y_true)\n",
    "    pred_pairs = get_pairs(y_pred)\n",
    "    \n",
    "    tp = len(true_pairs & pred_pairs)\n",
    "    fp = len(pred_pairs - true_pairs)\n",
    "    fn = len(true_pairs - pred_pairs)\n",
    "\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f_score = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    \n",
    "    return precision, recall, f_score\n",
    "\n",
    "def evaluate_clustering(y_true, y_pred):\n",
    "    \n",
    "    # remapping \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    assert y_true.shape == y_pred.shape\n",
    "\n",
    "    labels = np.unique(y_true)\n",
    "    pred_labels = np.unique(y_pred)\n",
    "    cost_matrix = -contingency_matrix(y_true, y_pred)\n",
    "\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    best_mapping = {pred_labels[col]: labels[row] for row, col in zip(row_ind, col_ind)}\n",
    "\n",
    "    y_pred_mapped = np.array([best_mapping[label] for label in y_pred])\n",
    "\n",
    "    # evaluate\n",
    "    acc = cluster_accuracy(y_true, y_pred_mapped)\n",
    "    nmi = normalized_mutual_info_score(y_true, y_pred)\n",
    "    purity = purity_score(y_true, y_pred_mapped)\n",
    "    precision, recall, f_score = pairwise_precision_recall_fscore(y_true, y_pred_mapped)\n",
    "    ari = adjusted_rand_score(y_true, y_pred_mapped)\n",
    "\n",
    "    return {\n",
    "        \"ACC\": acc,\n",
    "        \"NMI\": nmi,\n",
    "        \"Purity\": purity,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F-score\": f_score,\n",
    "        \"ARI\": ari\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ACC': 0.148, 'NMI': 0.049214620507958426, 'Purity': 0.163, 'Precision': 0.10451862458951827, 'Recall': 0.40272864321608043, 'F-score': 0.1659649900288057, 'ARI': 0.009371416335532866}\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate_clustering(labels, global_graph_labels)\n",
    "print(metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
