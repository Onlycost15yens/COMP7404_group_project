{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COIL-20 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage.feature import local_binary_pattern\n",
    "from skimage.filters import gabor\n",
    "from skimage.transform import resize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intensity features (View 1) shape: (1440, 1024)\n",
      "LBP features (View 2) shape: (1440, 3304)\n",
      "Gabor features (View 3) shape: (1440, 6750)\n",
      "Labels shape: (1440,)\n"
     ]
    }
   ],
   "source": [
    "def extract_intensity_feature(img):\n",
    "    \"\"\"\n",
    "    提取强度特征：\n",
    "      - 将图像转换为灰度图；\n",
    "      - 将图像调整大小为 32x32；\n",
    "      - 展平为一个 1024-D 的向量（32x32=1024）。\n",
    "    \"\"\"\n",
    "    gray_img = img.convert(\"L\")\n",
    "    resized_img = resize(np.array(gray_img), (32, 32), anti_aliasing=True)\n",
    "    feature = resized_img.flatten()  # 1024-D 向量\n",
    "    return feature\n",
    "\n",
    "def extract_lbp_feature(img, desired_dim=3304):\n",
    "    \"\"\"\n",
    "    提取 LBP (局部二值模式) 特征：\n",
    "      - 将图像转换为灰度图；\n",
    "      - 计算灰度图的 LBP 图像（使用 uniform 模式）；\n",
    "      - 将图像划分为不重叠的块，并计算每个块的直方图；\n",
    "      - 拼接所有直方图，并通过截断或填充将向量调整为 desired_dim（3304）维。\n",
    "    \n",
    "    注意：这里使用简单的块直方图策略，实际情况可根据需要调整块大小和直方图的 bin 数。\n",
    "    \"\"\"\n",
    "    gray = np.array(img.convert(\"L\"))\n",
    "    radius = 1\n",
    "    n_points = 8\n",
    "    lbp_image = local_binary_pattern(gray, P=n_points, R=radius, method=\"uniform\")\n",
    "    \n",
    "    block_size = 8  # 根据需求调整块大小\n",
    "    h, w = lbp_image.shape\n",
    "    features = []\n",
    "    n_bins = n_points + 2  # 通常 uniform 模式下有 n_points+2 个 bin\n",
    "    for i in range(0, h, block_size):\n",
    "        for j in range(0, w, block_size):\n",
    "            block = lbp_image[i:i+block_size, j:j+block_size]\n",
    "            hist, _ = np.histogram(block, bins=np.arange(0, n_bins+1), density=True)\n",
    "            features.extend(hist)\n",
    "    \n",
    "    feature_vec = np.array(features)\n",
    "    if feature_vec.shape[0] > desired_dim:\n",
    "        feature_vec = feature_vec[:desired_dim]\n",
    "    elif feature_vec.shape[0] < desired_dim:\n",
    "        feature_vec = np.pad(feature_vec, (0, desired_dim - feature_vec.shape[0]), mode=\"constant\")\n",
    "    return feature_vec\n",
    "\n",
    "def extract_gabor_feature(img, desired_dim=6750):\n",
    "    \"\"\"\n",
    "    提取 Gabor 特征：\n",
    "      - 将图像转换为灰度图；\n",
    "      - 使用不同频率和方向的一组 Gabor 滤波器处理图像；\n",
    "      - 对每个滤波器计算幅值响应；\n",
    "      - 将每个响应图划分为块，计算块的直方图；\n",
    "      - 拼接直方图，并通过填充或截断调整维度为 desired_dim（6750）。\n",
    "    \n",
    "    注意：这里的参数（滤波器的频率、方向、块大小、直方图的 bin 数）均为示例，实际应用中可能需要根据实验结果调整。\n",
    "    \"\"\"\n",
    "    gray = np.array(img.convert(\"L\"))\n",
    "    \n",
    "    frequencies = [0.1, 0.2, 0.3]\n",
    "    thetas = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "    \n",
    "    features = []\n",
    "    for frequency in frequencies:\n",
    "        for theta in thetas:\n",
    "            filt_real, filt_imag = gabor(gray, frequency=frequency, theta=theta)\n",
    "            magnitude = np.sqrt(filt_real**2 + filt_imag**2)\n",
    "            resized_mag = resize(magnitude, (64, 64), anti_aliasing=True)\n",
    "            \n",
    "            block_size = 8  # 根据需要调整块大小\n",
    "            n_bins = 10     # 直方图的 bin 数\n",
    "            for i in range(0, 64, block_size):\n",
    "                for j in range(0, 64, block_size):\n",
    "                    block = resized_mag[i:i+block_size, j:j+block_size]\n",
    "                    hist, _ = np.histogram(block, bins=n_bins, density=True)\n",
    "                    features.extend(hist)\n",
    "    \n",
    "    feature_vec = np.array(features)\n",
    "    if feature_vec.shape[0] > desired_dim:\n",
    "        feature_vec = feature_vec[:desired_dim]\n",
    "    elif feature_vec.shape[0] < desired_dim:\n",
    "        feature_vec = np.pad(feature_vec, (0, desired_dim - feature_vec.shape[0]), mode=\"constant\")\n",
    "    return feature_vec\n",
    "\n",
    "### 2. 处理 dataset 文件夹下的 COIL-20 图像，并提取三视图特征\n",
    "\n",
    "def process_coil20(dataset_dir):\n",
    "    \"\"\"\n",
    "    处理 dataset 目录中所有 PNG 图像：\n",
    "      - 读取每一张图像；\n",
    "      - 分别提取强度特征（1024-D）、LBP 特征（3304-D）和 Gabor 特征（6750-D）；\n",
    "      - 返回 3 个 numpy 数组（每个视图一个）以及对应的标签。\n",
    "    \n",
    "    假设图像命名格式为 \"objXX__YY.png\"，其中 XX 为目标类别标签。\n",
    "    \"\"\"\n",
    "    # 获取 dataset 目录下所有 PNG 文件\n",
    "    image_paths = glob.glob(os.path.join(dataset_dir, \"*.png\"))\n",
    "    intensity_features = []\n",
    "    lbp_features = []\n",
    "    gabor_features = []\n",
    "    labels = []\n",
    "    \n",
    "    for path in sorted(image_paths):\n",
    "        try:\n",
    "            img = Image.open(path)\n",
    "        except Exception as e:\n",
    "            print(f\"读取图像 {path} 出错：{e}\")\n",
    "            continue\n",
    "        \n",
    "        view1 = extract_intensity_feature(img)  # 1024-D\n",
    "        view2 = extract_lbp_feature(img)         # 3304-D\n",
    "        view3 = extract_gabor_feature(img)       # 6750-D\n",
    "        \n",
    "        intensity_features.append(view1)\n",
    "        lbp_features.append(view2)\n",
    "        gabor_features.append(view3)\n",
    "        \n",
    "        # 从文件名中提取标签，例如 \"obj10__5.png\" 则标签为 10\n",
    "        filename = os.path.basename(path)\n",
    "        label_str = filename.split(\"__\")[0]\n",
    "        label = int(''.join(filter(str.isdigit, label_str)))\n",
    "        labels.append(label)\n",
    "    \n",
    "    intensity_features = np.array(intensity_features)\n",
    "    lbp_features = np.array(lbp_features)\n",
    "    gabor_features = np.array(gabor_features)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return intensity_features, lbp_features, gabor_features, labels\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 假设 dataset 文件夹与当前代码文件在同一路径下\n",
    "    dataset_dir = \"./dataset\"\n",
    "    view1_features, view2_features, view3_features, labels = process_coil20(dataset_dir)\n",
    "    \n",
    "    # 打印提取特征的维度，验证是否符合预期\n",
    "    print(\"Intensity features (View 1) shape:\", view1_features.shape)  # 预期形状: (n_samples, 1024)\n",
    "    print(\"LBP features (View 2) shape:\", view2_features.shape)         # 预期形状: (n_samples, 3304)\n",
    "    print(\"Gabor features (View 3) shape:\", view3_features.shape)         # 预期形状: (n_samples, 6750)\n",
    "    print(\"Labels shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View 1 shape: (1440, 1024)\n",
      "View 2 shape: (1440, 3304)\n",
      "View 3 shape: (1440, 6750)\n"
     ]
    }
   ],
   "source": [
    "features_list = [view1_features, view2_features, view3_features]\n",
    "for idx, view in enumerate(features_list):\n",
    "    print(f\"View {idx+1} shape:\", view.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import eigh\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_simplex(v):\n",
    "    n = len(v)\n",
    "    u = np.sort(v)[::-1]\n",
    "    cssv = np.cumsum(u)\n",
    "    rho = np.nonzero(u + (1 - cssv) / (np.arange(n) + 1) > 0)[0][-1]\n",
    "    theta = (cssv[rho] - 1) / (rho + 1)\n",
    "    w = np.maximum(v - theta, 0)\n",
    "    return w\n",
    "\n",
    "def compute_laplacian(S):\n",
    "    D = np.diag(S.sum(axis=1))\n",
    "    L = D - S\n",
    "    return L\n",
    "\n",
    "def update_S(Q, beta):\n",
    "    n, c = Q.shape\n",
    "    S = np.zeros((n, n))\n",
    "    Q_norms = np.sum(Q**2, axis=1, keepdims=True)\n",
    "    dist_sq = Q_norms + Q_norms.T - 2 * np.dot(Q, Q.T)\n",
    "    dist_sq = np.maximum(dist_sq, 0)\n",
    "    \n",
    "    for j in range(n):\n",
    "        g_j = dist_sq[:, j]\n",
    "        v = -g_j / (2 * beta)\n",
    "        s_j = project_simplex(v)\n",
    "        S[:, j] = s_j\n",
    "        \n",
    "    return S\n",
    "\n",
    "def update_Q(L, c):\n",
    "    eigvals, eigvecs = eigh(L)\n",
    "    Q = eigvecs[:, :c]\n",
    "    return Q, eigvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_single_view_graph(single_view_graph_X, class_number, default_beta=1.0):\n",
    "    \n",
    "    single_view_graph = []\n",
    "    \n",
    "    for i in tqdm(range(len(single_view_graph_X))):\n",
    "        \n",
    "        # init\n",
    "        beta = default_beta\n",
    "        S = update_S(single_view_graph_X[i], beta)\n",
    "        S = (S + S.T) / 2\n",
    "        L = compute_laplacian(S)\n",
    "        Q = update_Q(L, class_number)\n",
    "        Q, eigvals = update_Q(L, class_number)\n",
    "\n",
    "        for j in range(100):\n",
    "            S = update_S(Q, beta)\n",
    "            L = compute_laplacian(S)\n",
    "            Q_old = Q\n",
    "            Q, eigvals = update_Q(L, class_number)\n",
    "\n",
    "            fn1 = np.sum(eigvals[:class_number])\n",
    "            fn2 = np.sum(eigvals[:class_number+1])\n",
    "            # print(\"L_rank\",L_rank, \"beta\", beta)\n",
    "            if fn1 > 0.00000000001:\n",
    "                beta = beta * 2\n",
    "                #tqdm.write(f\"{i}th graph end at {j}th iteration, smallest eigenvalues: {eigvals[:class_number]}\")\n",
    "            elif fn2 < 0.00000000001:\n",
    "                beta = beta / 2\n",
    "                Q = Q_old\n",
    "                #tqdm.write(f\"{i}th graph end at {j}th iteration, smallest eigenvalues: {eigvals[:class_number]}\")\n",
    "            else:\n",
    "                num_zero_eig = np.sum(eigvals < 1e-5)\n",
    "                tqdm.write(f\"Converged: the Laplacian has {num_zero_eig} (>= {class_number}) zero eigenvalues.\")\n",
    "                break\n",
    "                \n",
    "        single_view_graph.append(S)\n",
    "        \n",
    "    return single_view_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [03:30<00:00, 70.08s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged: the Laplacian has 20 (>= 20) zero eigenvalues.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class_number = 20\n",
    "single_view_graph = make_single_view_graph(features_list, class_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_graph_learning(S_list, c, gamma=1.0, max_iter=100, tol=1e-4):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        S_list  : a list with each element being a single-view graph (n x n matrix), total of nv views.\n",
    "        c       : number of clusters (global graph A should have exactly c connected components).\n",
    "        gamma   : trade-off parameter in the global graph objective function.\n",
    "        max_iter: maximum number of iterations.\n",
    "        tol     : tolerance for convergence.\n",
    "\n",
    "    Output:\n",
    "        A       : the learned global graph (n x n matrix)\n",
    "        P       : spectral embedding matrix (n x c matrix)\n",
    "        W       : weight matrix (nv x n) where each column corresponds to the weights for each view for a data point.\n",
    "    \"\"\"\n",
    "    nv = len(S_list)  # number of views\n",
    "    n = S_list[0].shape[0]  # number of data points\n",
    "\n",
    "    # Initialize: set all view weights for each data point to 1/nv\n",
    "    W = np.full((nv, n), 1.0 / nv)\n",
    "\n",
    "    # Initialize the global graph A using the weighted-sum rule from S_list\n",
    "    A = np.zeros((n, n))\n",
    "    for j in range(n):\n",
    "        # For the j-th column corresponding to data point j\n",
    "        a_j = np.zeros(n)\n",
    "        for v in range(nv):\n",
    "            a_j += W[v, j] * S_list[v][:, j]\n",
    "        A[:, j] = a_j\n",
    "    # Enforce symmetry for A\n",
    "    A = (A + A.T) / 2\n",
    "\n",
    "    # Compute the initial Laplacian and spectral embedding matrix P\n",
    "    L = compute_laplacian(A)\n",
    "    P, eigvals = update_Q(L, c)\n",
    "\n",
    "    # Main iterative process\n",
    "    for it in tqdm(range(max_iter), desc=\"Global Graph Learning\"):\n",
    "        A_old = A.copy()\n",
    "\n",
    "        # =============================\n",
    "        # 1. Update the global graph A: update each column a_j for data point j\n",
    "        # =============================\n",
    "        for j in range(n):\n",
    "            # Compute h_j, where h_j[i] = ||P[i, :] - P[j, :]||^2\n",
    "            p_j = P[j, :]\n",
    "            h_j = np.sum((P - p_j)**2, axis=1)\n",
    "            # Compute the weighted sum from single-view graphs: s_bar = sum_v W[v, j] * S_list[v][:, j]\n",
    "            s_bar = np.zeros(n)\n",
    "            for v in range(nv):\n",
    "                s_bar += W[v, j] * S_list[v][:, j]\n",
    "            # Construct the updating vector v_vec = s_bar - (gamma/2)*h_j\n",
    "            v_vec = s_bar - (gamma / 2.0) * h_j\n",
    "            # Update a_j by projecting v_vec onto the probability simplex\n",
    "            A[:, j] = project_simplex(v_vec)\n",
    "\n",
    "        # Enforce symmetry for A\n",
    "        A = (A + A.T) / 2\n",
    "\n",
    "        # =============================\n",
    "        # 2. Update the spectral embedding matrix P\n",
    "        # =============================\n",
    "        L = compute_laplacian(A)\n",
    "        P, eigvals = update_Q(L, c)\n",
    "\n",
    "        # Check condition: if the Laplacian L has at least c zero eigenvalues, we assume the ideal graph structure is reached.\n",
    "        num_zero_eig = np.sum(eigvals < 1e-5)\n",
    "        if num_zero_eig >= c:\n",
    "            tqdm.write(f\"Converged at iteration {it}: {num_zero_eig} zero eigenvalues (>= {c}).\")\n",
    "            break\n",
    "\n",
    "        # =============================\n",
    "        # 3. Update the weight matrix W\n",
    "        # For each data point j, update its view weights based on the difference between A and each S_list[:, j].\n",
    "        # =============================\n",
    "        for j in range(n):\n",
    "            # Construct matrix Z_j of size n x nv: the v-th column is A[:, j] - S_list[v][:, j]\n",
    "            Z_j = np.column_stack([A[:, j] - S_list[v][:, j] for v in range(nv)])\n",
    "            M = np.dot(Z_j.T, Z_j)  # size nv x nv\n",
    "            # Add a small regularizer for numerical stability\n",
    "            reg = 1e-8 * np.eye(nv)\n",
    "            try:\n",
    "                M_inv = np.linalg.inv(M + reg)\n",
    "            except np.linalg.LinAlgError:\n",
    "                M_inv = np.linalg.pinv(M + reg)\n",
    "            ones = np.ones((nv, 1))\n",
    "            w_j = np.dot(M_inv, ones)\n",
    "            w_j = w_j / np.sum(w_j)  # normalize so the sum equals 1\n",
    "            # Update the j-th column of weight matrix W\n",
    "            for v in range(nv):\n",
    "                W[v, j] = w_j[v, 0]\n",
    "\n",
    "        # Optionally: check if A has converged by monitoring relative change\n",
    "        if np.linalg.norm(A - A_old, 'fro') / np.linalg.norm(A_old, 'fro') < tol:\n",
    "            tqdm.write(f\"A converged at iteration {it} with relative change below {tol}.\")\n",
    "            break\n",
    "\n",
    "    return A\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_W(single_view_graph):\n",
    "    W = [np.full(single_view_graph[0].shape, 1/len(single_view_graph))] * len(single_view_graph)\n",
    "    return W\n",
    "\n",
    "def init_A(single_view_graph, W):\n",
    "    A = np.sum(single_view_graph, axis=0) * W[0]\n",
    "    return A\n",
    "\n",
    "def init_P(A,c):\n",
    "    L = compute_laplacian(A)\n",
    "    P, eigvals = update_Q(L, c)\n",
    "    return P, eigvals\n",
    "\n",
    "def update_A(P, w_list, s_list, gamma=1.0):\n",
    "    n = P.shape[0]\n",
    "    c = P.shape[1]\n",
    "    m = len(w_list)\n",
    "\n",
    "    H = np.sum((P[:, np.newaxis, :] - P)**2, axis=2)\n",
    "    \n",
    "    A = np.zeros((n, n))\n",
    "    \n",
    "    for j in range(c):\n",
    "        h_j = H[:, j]\n",
    "    \n",
    "        sum_term = np.zeros(n)\n",
    "        for v in range(m):\n",
    "            w_jv = w_list[v][:, j]\n",
    "            s_jv = s_list[v][:, j] \n",
    "            sum_term += w_jv * s_jv  \n",
    "        intermediate = (((gamma / 2.0) * (h_j)) - sum_term)\n",
    "        \n",
    "    eta = (1 + np.sum(intermediate)) / n\n",
    "    a_j = np.maximum(0, -intermediate + eta)\n",
    "    A[j] = a_j\n",
    "\n",
    "    return A\n",
    "\n",
    "\n",
    "def update_P(L, c):\n",
    "    eigvals, eigenvectors = np.linalg.eigh(L)\n",
    "    Q = eigenvectors[:, :c]\n",
    "    return Q, eigvals\n",
    "\n",
    "\n",
    "def compute_W(a, s_list):\n",
    "    v, n, _ = np.shape(s_list) \n",
    "    w_list = []\n",
    "\n",
    "    for i in range(v):\n",
    "        wv = np.zeros((n, n)) \n",
    "        for j in range(n):\n",
    "            Z_j = a[:,j] - s_list[i][:,j]\n",
    "            Z_j = Z_j.reshape(1, -1) \n",
    "            one_vector = np.ones((n, 1)) \n",
    "            \n",
    "            # try: # takes forever\n",
    "            #     print(\"not triggered\", j)\n",
    "            #     ZTZ_inv = np.linalg.pinv(Z_j.T @ Z_j)  # (Z_j^T Z_j)^{-1}\n",
    "            #     w_jv = (ZTZ_inv @ one_vector) * (1 / (one_vector.T @ ZTZ_inv @ one_vector))\n",
    "            # except:\n",
    "            #     print(\"triggered\", j)\n",
    "            #     w_jv = np.zeros((1, n))\n",
    "                \n",
    "            sum_Z = np.sum(Z_j)\n",
    "            if np.isclose(sum_Z, 0.0):\n",
    "                w_jv = np.zeros((1, n))\n",
    "            else:\n",
    "                w_jv = Z_j.T / sum_Z\n",
    "\n",
    "            wv[:,j] = w_jv.reshape(-1) / np.sum(w_jv)\n",
    "        w_list.append(wv)\n",
    "\n",
    "    return w_list\n",
    "\n",
    "def make_global_graph(single_view_graph, class_number, default_gamma=1.0):\n",
    "    \n",
    "    # init\n",
    "    W = init_W(single_view_graph)\n",
    "    A = init_A(single_view_graph, W)\n",
    "    P, eigvals= init_P(A, class_number)\n",
    "    gamma = default_gamma\n",
    "    \n",
    "    for j in tqdm(range(100)):\n",
    "        A = update_A(P, W, single_view_graph)\n",
    "        L = compute_laplacian(A)\n",
    "        P, eigvals = update_P(L, class_number)\n",
    "        W = compute_W(A, single_view_graph)\n",
    "        fn1 = np.sum(eigvals[:class_number])\n",
    "        fn2 = np.sum(eigvals[:class_number+1])\n",
    "        # print(\"L_rank\",L_rank, \"beta\", beta)\n",
    "        if fn1 > 0.00000000001:\n",
    "            gamma = gamma * 2\n",
    "            #tqdm.write(f\"{i}th graph end at {j}th iteration, smallest eigenvalues: {eigvals[:class_number]}\")\n",
    "        elif fn2 < 0.00000000001:\n",
    "            gamma = gamma / 2\n",
    "            #tqdm.write(f\"{i}th graph end at {j}th iteration, smallest eigenvalues: {eigvals[:class_number]}\")\n",
    "        else:\n",
    "            num_zero_eig = np.sum(eigvals < 1e-5)\n",
    "            tqdm.write(f\"Converged: the Laplacian has {num_zero_eig} (>= {class_number}) zero eigenvalues.\")\n",
    "            break\n",
    "        \n",
    "    return L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:37<00:00,  1.03it/s]\n"
     ]
    }
   ],
   "source": [
    "global_graph = make_global_graph(single_view_graph, class_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def cluster(laplacian, n_clusters):\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(laplacian)\n",
    "    X = eigenvectors[:, :n_clusters]\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(X)\n",
    "    return kmeans.labels_\n",
    "\n",
    "# get clustering results\n",
    "single_view_graph_labels = []\n",
    "for i in range(len(single_view_graph)):\n",
    "    single_view_graph_labels.append(cluster(single_view_graph[i], class_number))\n",
    "\n",
    "global_graph_labels = cluster(global_graph, class_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "\n",
    "def cluster_accuracy(y_true, y_pred):\n",
    "    acc = np.mean(y_pred == y_true)\n",
    "    return acc\n",
    "\n",
    "def purity_score(y_true, y_pred):\n",
    "    contingency = contingency_matrix(y_true, y_pred)\n",
    "    return np.sum(np.amax(contingency, axis=0)) / np.sum(contingency)\n",
    "\n",
    "def pairwise_precision_recall_fscore(y_true, y_pred):\n",
    "\n",
    "    def get_pairs(labels):\n",
    "        pairs = set()\n",
    "        for label in np.unique(labels):\n",
    "            indices = np.where(labels == label)[0]\n",
    "            for i in range(len(indices)):\n",
    "                for j in range(i + 1, len(indices)):\n",
    "                    pairs.add((indices[i], indices[j]))\n",
    "        return pairs\n",
    "\n",
    "    true_pairs = get_pairs(y_true)\n",
    "    pred_pairs = get_pairs(y_pred)\n",
    "    \n",
    "    tp = len(true_pairs & pred_pairs)\n",
    "    fp = len(pred_pairs - true_pairs)\n",
    "    fn = len(true_pairs - pred_pairs)\n",
    "\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f_score = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "    \n",
    "    return precision, recall, f_score\n",
    "\n",
    "def evaluate_clustering(y_true, y_pred):\n",
    "    \n",
    "    # remapping \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    assert y_true.shape == y_pred.shape\n",
    "\n",
    "    labels = np.unique(y_true)\n",
    "    pred_labels = np.unique(y_pred)\n",
    "    cost_matrix = -contingency_matrix(y_true, y_pred)\n",
    "\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    best_mapping = {pred_labels[col]: labels[row] for row, col in zip(row_ind, col_ind)}\n",
    "\n",
    "    y_pred_mapped = np.array([best_mapping[label] for label in y_pred])\n",
    "\n",
    "    # evaluate\n",
    "    acc = cluster_accuracy(y_true, y_pred_mapped)\n",
    "    nmi = normalized_mutual_info_score(y_true, y_pred)\n",
    "    purity = purity_score(y_true, y_pred_mapped)\n",
    "    precision, recall, f_score = pairwise_precision_recall_fscore(y_true, y_pred_mapped)\n",
    "    ari = adjusted_rand_score(y_true, y_pred_mapped)\n",
    "\n",
    "    return {\n",
    "        \"ACC\": acc,\n",
    "        \"NMI\": nmi,\n",
    "        \"Purity\": purity,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F-score\": f_score,\n",
    "        \"ARI\": ari\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ACC': 0.050694444444444445, 'NMI': 0.026641223449763655, 'Purity': 0.06319444444444444, 'Precision': 0.049500946566096084, 'Recall': 0.9769561815336463, 'F-score': 0.0942275218625888, 'ARI': 0.000338506565232005}\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate_clustering(labels, global_graph_labels)\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
